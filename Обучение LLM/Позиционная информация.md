Зачем вообще нужны «позиции»  
Механизм self-attention видит предложение как мешок отдельных векторов.  
Если не подсказать, какое слово стоит первым, вторым, третьим, он легко перепутает «Кот ест рыбу» и «Рыбу ест кот».  
Поэтому каждому токену добавляют (или как-то вшивают) «ярлык-координату».

Ниже три популярные семейства «ярлыков» – как они устроены и чем отличаются.
1. Синусоидальные позиции (оригинальный Transformer) Идея  
• Для каждой позиции 0, 1, 2… вычисляем набор синусов и косинусов разных «частот».  
• Получается длинный волнистый штрих-код: ни один номер не повторяется, а сдвиг на k позиций превращается просто в сдвиг фаз волн.

Как делается на практике  
1. Заранее по формуле sin() / cos() строим таблицу размера (максимальная_длина, ширина_вектора).  
2. При обучении берём вектор нужной строки и прибавляем его к эмбеддингу слова.  
3. Весов, которые нужно учить, здесь НЕТ – всё задаётся математической формулой.

Плюсы / минусы  
+ Работает на любой длине текста: формулу можно продолжать бесконечно.  
+ Даёт модели понятие «расстояние между словами».  
− Нельзя подогнать «рисунок волн» к конкретной задаче – он фиксирован.

2. Learnable абсолютные позиции (пример – BERT) Идея  
• Храним таблицу, где для «0-го места», «1-го места», … лежат обычные обучаемые векторы.  
• Это почти то же, что таблица word-эмбеддингов, только индексы – номера позиций.

Как делается  
1. Задаём максимальную длину, скажем 512.  
2. Создаём матрицу 512 × d_model.  
3. Во время обучения вместе со словами учатся и эти позиционные векторы.

Плюсы / минусы  
+ Модель сама подбирает удобный «код» для конкретной задачи.  
− Таблица обрезана сверху. Если внезапно нужно обработать 1024 токена, ничего «за 512-й» не обучено.  
− Перенос на длины, которых не было в обучении, даётся плохо.

3. Относительные позиции (Transformer-XL, ALiBi, RoPE…) Главная мысль  
Модели важнее не «я стою 137-м», а «я стою на 5 слов правее тебя».  
Поэтому вместо абсолютного номера кодируют расстояние между двумя токенами внутри внимания.

Ниже – три варианта, как именно это делают.

a) Transformer-XL (и T5-relative, DeBERTa)  
• Для каждого расстояния d = −k … +k есть маленький вектор или смещение.  
• При расчёте attention i → j к скалярному произведению добавляют вектор, соответствующий (j − i).  
• Умеет «цеплять» зависимость на любые длины, если для больших d используют один и тот же «дальний» код.

b) ALiBi (Attention with Linear Biases)  
• Вообще не добавляем отдельный вектор к токену.  
• При вычислении «похожести» просто вычитаем коэффициент·distance.  
У каждой головы свой коэффициент (наклон).  
• Максимально дёшево по памяти и работает на сколь угодно длинных текстах.

c) RoPE (Rotary Positional Embedding, GPT-NeoX, LLaMA)  
• Берём пары координат (x, y) внутри вектора Query/Key и «поворачиваем» их на угол, равный номеру позиции.  
• После поворота скалярное произведение двух векторов зависит только от разницы углов → то есть от относительного смещения.  
• Плюс – не нужно хранить большую таблицу; код «продолжается» за пределы обученной длины.

Почему относительные схемы любят длинные тексты  
• Нет жёсткого верхнего лимита длины.  
• Модель учится «хватать» зависимость, если слова стоят на расстоянии 2, 5 или 1000 токенов, и делает это одинаково уверенно как на тренировке, так и на инференсе.  
• Память экономнее, потому что не надо держать огромный словарь позиций.

Короткая сводка 
Синусы – фиксированный «штрих-код», работает на любых длинах, но неизменяем.  
Learnable абсолютные – таблица, которую модель подгоняет под данные, но она ограничена заранее выбранной длиной.  
Относительные (Transformer-XL, ALiBi, RoPE) – кодируют именно «расстояние», легче обрабатывают и обучаются на очень длинных текстах; RoPE делает это «поворотом векторов», ALiBi – простым линейным уклоном, а Transformer-XL – отдельными эмбеддингами расстояний.